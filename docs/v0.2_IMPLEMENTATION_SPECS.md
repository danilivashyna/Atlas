# v0.2 Implementation Specifications

> **Last Updated**: 2025-10-20
> **Format**: Copy-paste briefs for VS Code Copilot agents
> **All features**: Offline-friendly, pytest -q passing, UTC timestamps, Pydantic v2

---

## ðŸŽ¯ Universal Rules for All Tasks

**Repository**: danilivashyna/Atlas
**Style**: Black + isort + mypy (where type hints exist)
**CI Requirements**: No network/downloads; everything offline
**Timestamps**: UTC ISO-8601 only
**Pydantic**: Use ConfigDict (v2)
**Testing**: `pytest -q` locally + smoke tests in PR
**Formatter**: Before push run:
```bash
python -m pip install black isort
black .
isort .
pytest -q
```

---

## PR #7 â€” v0.2-03: Hierarchical API Endpoints

**Branch**: `feature/v0.2-03-api-hier-ops`

### Quick Brief (Copilot)
```
Repository: danilivashyna/Atlas
Branch: feature/v0.2-03-api-hier-ops
Goal: add POST /encode_h, /decode_h, /explain_h with Pydantic v2 models; update OpenAPI.
Deliverables: endpoints in app.py, models.py; smoke tests; docs.
Acceptance: pytest -q green; /docs shows 3 routes; responses include trace_id + UTC timestamp.
```

### Files to Modify
- `src/atlas/api/app.py` â€” Add 3 new FastAPI routes with "Hierarchical" tags
- `src/atlas/api/models.py` â€” Pydantic v2 schemas: EncodeHRequest, EncodeHResponse, DecodeHRequest, DecodeHResponse, ExplainHRequest, ExplainHResponse
- `docs/HIERARCHICAL_API.md` â€” Payload/response examples
- `tests/test_api_smoke.py` â€” Smoke tests for 3 routes

### Acceptance Criteria
- âœ… `pytest -q` passes all tests
- âœ… OpenAPI correctly describes all parameters and examples
- âœ… `/docs` (Swagger UI) renders 3 new endpoints
- âœ… Responses include `trace_id` (UUID) and `timestamp` (UTC ISO 8601 "Z" format)
- âœ… Pydantic v2 validation working (ConfigDict, field validators)

### How to Test
```bash
# Terminal 1: Start API
uvicorn src.atlas.api.app:app --port 8010 &

# Terminal 2: Test endpoints
curl -s http://127.0.0.1:8010/encode_h \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"text":"hello world"}'
# Expected: {"embedding": [5D array], "trace_id": "...", "timestamp": "...Z"}

curl -s http://127.0.0.1:8010/decode_h \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"embedding":[1,0.5,-0.3,0.2,-0.1]}'
# Expected: {"text": "...", "trace_id": "...", "timestamp": "...Z"}

# Run smoke tests
pytest tests/test_api_smoke.py -q -k "encode_h or decode_h or explain_h"

# View OpenAPI
open http://127.0.0.1:8010/docs
```

### Implementation Notes
- Stubs acceptable for business logic (implement in v0.2.1 if needed)
- Use `uuid.uuid4()` for trace_id
- Timestamps: `datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')`
- Responses can be simple initially (focus on API contract)

---

## PR #8 â€” v0.2-04: Hierarchical Losses

**Branch**: `feature/v0.2-04-losses-hier`

### Quick Brief (Copilot)
```
Repository: danilivashyna/Atlas
Branch: feature/v0.2-04-losses-hier
Goal: implement Orthogonality, Sparsity(L1), RouterEntropy losses.
Deliverables: losses.py + tests + docs.
Acceptance: numeric tests pass; no NaN/Inf; pytest -q green.
```

### Files to Modify
- `src/atlas/training/losses.py` â€” Implement 3 loss classes
- `tests/test_losses.py` â€” Numerical correctness tests, edge cases
- `docs/LOSS_FUNCTIONS_v0.2.md` â€” Formulas and examples

### Loss Functions Details

#### 1. OrthogonalityLoss
```python
# Input: W shape (D, 5) â€” encoder weights or similar
# Formula: ||W^T W - I||_F^2 / (5^2)
# Ensures columns are orthogonal (normalized)
```

#### 2. SparsityLoss (L1)
```python
# Input: embeddings shape (batch, 5)
# Formula: mean(|embeddings|) or sum(|embeddings|) / batch_size
# Encourages sparse 5D representations
```

#### 3. RouterEntropyLoss
```python
# Input: router logits or attention weights shape (batch, K)
# Formula: -sum(p * log(p + eps)) where p = softmax(logits)
# Prevents mode collapse (keeps all routes active)
```

### Acceptance Criteria
- âœ… `pytest -q` passes all tests
- âœ… No NaN/Inf on valid inputs
- âœ… Numerical gradient checks passing (optional but good)
- âœ… Tests deterministic (no random behavior)
- âœ… Type hints complete, docstrings present

### How to Test
```bash
pytest tests/test_losses.py -v

# Manual verification
python -c "
from src.atlas.training.losses import OrthogonalityLoss, SparsityLoss, RouterEntropyLoss
import numpy as np

# Test orthogonality
W = np.random.randn(10, 5)
loss = OrthogonalityLoss()(W)
print(f'Orthogonality loss: {loss}')  # Should be scalar, not NaN

# Test sparsity
embeddings = np.random.randn(32, 5)
loss = SparsityLoss()(embeddings)
print(f'Sparsity loss: {loss}')  # Should be scalar

# Test entropy
logits = np.random.randn(32, 10)
loss = RouterEntropyLoss()(logits)
print(f'Entropy loss: {loss}')  # Should be scalar
"
```

### Implementation Notes
- Use NumPy or PyTorch (either is fine, but NumPy preferred for simplicity)
- Ensure numerical stability (add small epsilon to prevent log(0))
- Losses should be framework-agnostic
- No external dependencies beyond NumPy/PyTorch

---

## PR #9 â€” v0.2-05: Teacher â†’ Student Distillation

**Branch**: `feature/v0.2-05-distill-teacher`

### Quick Brief (Copilot)
```
Repository: danilivashyna/Atlas
Branch: feature/v0.2-05-distill-teacher
Goal: distillation scaffold with CLI; offline friendly.
Deliverables: distill.py, configs.py, tools/train_distill.py, docs, smoke test.
Acceptance: dry-run steps on CPU; pytest -q green; CLI --help works.
```

### Files to Modify
- `src/atlas/training/distill.py` â€” `Distiller` class with methods
- `src/atlas/training/configs.py` â€” `DistillerConfig` dataclass
- `tools/train_distill.py` â€” CLI tool
- `docs/distillation_quickstart.md` â€” Quick start guide
- `tests/test_distill_smoke.py` â€” Dry-run tests

### Distiller Class API
```python
class Distiller:
    def __init__(self, config: DistillerConfig):
        pass

    def load_teacher(self, path_or_name: str):
        """Load teacher model from path or use dummy"""
        pass

    def load_student(self, path_or_name: str):
        """Load student model from path or use dummy"""
        pass

    def step(self, batch: dict) -> dict:
        """Single training step; return loss dict"""
        pass

    def train_loop(self, num_epochs: int, train_loader):
        """Multi-epoch training loop"""
        pass

    def save(self, output_dir: str):
        """Save student + logs to directory"""
        pass
```

### DistillerConfig
```python
@dataclass
class DistillerConfig:
    teacher_path: str = "dummy"
    student_path: str = "dummy"
    learning_rate: float = 1e-4
    temperature: float = 4.0  # For KL divergence
    batch_size: int = 32
    num_epochs: int = 3
    output_dir: str = "./runs"
```

### CLI Usage
```bash
python tools/train_distill.py --help
# Shows: --teacher, --student, --epochs, --batch-size, --output-dir, --lr

python tools/train_distill.py --epochs 2 --batch-size 16 --output-dir ./my_run
# Logs to stdout; saves artifacts to ./my_run/
```

### Acceptance Criteria
- âœ… `pytest -q` passes without network
- âœ… CLI works: `python tools/train_distill.py --help`
- âœ… Dry-run 1-2 steps execute on CPU without crashing
- âœ… Training logs printed to stdout
- âœ… Artifacts saved to `{output_dir}/`

### How to Test
```bash
pytest tests/test_distill_smoke.py -v

# Manual test
python tools/train_distill.py --help
python tools/train_distill.py --epochs 1 --batch-size 2 --output-dir ./test_distill
ls -la ./test_distill/  # Should have logs/artifacts
```

### Implementation Notes
- Use dummy/stubbed models in tests (no real downloads)
- KL divergence formula: `sum(p_teacher * (log(p_teacher) - log(p_student)))`
- Focus on training loop structure, not perfect accuracy
- All models can be fakes (np.random) for smoke tests

---

## PR #10 â€” v0.2-06: Hierarchical Metrics

**Branch**: `feature/v0.2-06-metrics-hier`

### Quick Brief (Copilot)
```
Repository: danilivashyna/Atlas
Branch: feature/v0.2-06-metrics-hier
Goal: implement H-Coherence, H-Stability, H-Diversity.
Deliverables: metrics_hier.py + tests + docs.
Acceptance: deterministic examples; values in valid ranges; pytest -q green.
```

### Files to Modify
- `src/atlas/metrics/metrics_hier.py` â€” Three metric functions
- `tests/test_metrics_hier.py` â€” Deterministic test cases
- `docs/METRICS_v0.2.md` â€” Formulas and interpretation

### Metrics Specifications

#### 1. H-Coherence
```python
def coherence(embeddings: np.ndarray) -> float:
    """
    Measure alignment with 5D semantic space.
    Input: (N, 5) array of embeddings
    Output: 0 â‰¤ score â‰¤ 1 (higher = better alignment)
    Formula: mean pairwise cosine similarity / theoretical_max
    """
    pass
```

#### 2. H-Stability
```python
def stability(embeddings: np.ndarray) -> float:
    """
    Measure consistency of encoding.
    Input: (N, 5) array
    Output: 0 â‰¤ score â‰¤ 1 (higher = more stable)
    Formula: 1 - (std_dev / mean_magnitude)
    """
    pass
```

#### 3. H-Diversity
```python
def diversity(embeddings: np.ndarray) -> float:
    """
    Measure spread in embedding space.
    Input: (N, 5) array
    Output: 0 â‰¤ score â‰¤ 1 (higher = more diverse)
    Formula: ratio of actual_span / theoretical_max_span
    """
    pass
```

### Acceptance Criteria
- âœ… `pytest -q` passes all tests
- âœ… Metric values in reasonable range (typically 0-1)
- âœ… Deterministic (same input â†’ same output)
- âœ… No external models or downloads
- âœ… NumPy-based, pure Python

### How to Test
```bash
pytest tests/test_metrics_hier.py -v

# Manual verification
python -c "
from src.atlas.metrics.metrics_hier import coherence, stability, diversity
import numpy as np

embeddings = np.array([
    [1, 0, 0, 0, 0],
    [0.9, 0.1, 0, 0, 0],
    [-1, 0, 0, 0, 0],
])

print(f'Coherence: {coherence(embeddings)}')
print(f'Stability: {stability(embeddings)}')
print(f'Diversity: {diversity(embeddings)}')
# All should be floats in [0, 1]
"
```

### Implementation Notes
- Metrics are simple mathematical formulas on NumPy arrays
- No ML models required
- Focus on interpretability (metrics should make intuitive sense)
- Include examples in docstrings

---

## PR #11 â€” v0.2-07: Benchmarks

**Branch**: `feature/v0.2-07-benchmarks`

### Quick Brief (Copilot)
```
Repository: danilivashyna/Atlas
Branch: feature/v0.2-07-benchmarks
Goal: add pytest-benchmark scenarios; skip if pkg missing.
Deliverables: test_bench_encoder.py, test_bench_decoder.py, docs.
Acceptance: tests run/skip cleanly; no flakes.
```

### Files to Modify
- `tests/benchmarks/test_bench_encoder.py` â€” Encoder latency benchmarks
- `tests/benchmarks/test_bench_decoder.py` â€” Decoder latency benchmarks
- `docs/BENCHMARKS.md` â€” How to run and interpret

### Benchmark Structure
```python
# tests/benchmarks/test_bench_encoder.py

import pytest

pytest_plugins = "benchmark"  # Load benchmark plugin

@pytest.mark.skipif(not _has_pytest_benchmark(), reason="pytest-benchmark not installed")
def test_encode_latency(benchmark):
    """Benchmark encoder on 100 texts"""
    from src.atlas.encoders import TextEncoder5D

    encoder = TextEncoder5D()
    texts = ["hello world"] * 100

    def encode_all():
        return [encoder.encode(t) for t in texts]

    result = benchmark(encode_all)
    # pytest-benchmark reports: min, max, mean, etc.
```

### Acceptance Criteria
- âœ… `pytest tests/benchmarks/ -q` runs without errors
- âœ… Tests skipped cleanly if pytest-benchmark not installed
- âœ… Benchmark results are deterministic (no flakes)
- âœ… Baseline numbers documented

### How to Test
```bash
# Without pytest-benchmark (skipped)
pytest tests/benchmarks/ -q
# Output: 2 skipped

# With pytest-benchmark installed
pip install pytest-benchmark
pytest tests/benchmarks/ -q
# Output: benchmark results showing min, max, mean time

# View comparison across runs
pytest tests/benchmarks/ --benchmark-compare
```

### Implementation Notes
- If pytest-benchmark not installed, tests should skip gracefully
- Use `@pytest.mark.skipif` to check for package
- Focus on realistic input sizes
- Baseline numbers for future optimization

---

## PR #12 â€” v0.2-08: Docs/Demos/CLI

**Branch**: `feature/v0.2-08-docs-demos-cli`

### Quick Brief (Copilot)
```
Repository: danilivashyna/Atlas
Branch: feature/v0.2-08-docs-demos-cli
Goal: update docs and add atlas_cli.py (encode/decode/explain).
Deliverables: CLI + docs + smoke test.
Acceptance: CLI works offline; pytest -q green.
```

### Files to Modify
- `tools/atlas_cli.py` â€” CLI script with 3 commands
- `README.md` â€” Updated with v0.2 features
- `docs/QUICK_REFERENCE.md` â€” Current examples
- `tests/test_cli_smoke.py` â€” Smoke tests

### CLI Commands
```bash
python tools/atlas_cli.py encode <text>
# Example: python tools/atlas_cli.py encode "hello world"
# Output: JSON with 5D embedding

python tools/atlas_cli.py decode <embedding_json>
# Example: python tools/atlas_cli.py decode "[1,0.5,-0.3,0.2,-0.1]"
# Output: Reconstructed text

python tools/atlas_cli.py explain <embedding_json>
# Example: python tools/atlas_cli.py explain "[1,0.5,-0.3,0.2,-0.1]"
# Output: Per-dimension reasoning
```

### Acceptance Criteria
- âœ… `pytest -q` passes all tests
- âœ… CLI works without network/external downloads
- âœ… `python tools/atlas_cli.py --help` displays all commands
- âœ… Examples in docs are copy-paste ready
- âœ… Smoke tests verify basic functionality

### How to Test
```bash
pytest tests/test_cli_smoke.py -v

# Manual tests
python tools/atlas_cli.py --help
python tools/atlas_cli.py encode "hello world"
python tools/atlas_cli.py decode "[1, 0.5, -0.3, 0.2, -0.1]"
python tools/atlas_cli.py explain "[1, 0.5, -0.3, 0.2, -0.1]"
```

### Implementation Notes
- CLI should be user-friendly (good error handling)
- Output as pretty JSON
- No external model downloads
- Include examples in `--help` output

---

## ðŸ”§ Pre-Push Checklist (All PRs)

Before pushing to GitHub:

```bash
# 1. Format code
python -m pip install black isort
black .
isort .

# 2. Run all tests
pytest -q

# 3. Check for warnings
pytest -q 2>&1 | grep -i warning
# Should be empty

# 4. Verify no network calls (check test mocks)
# grep -r "requests\|http\|urllib" tests/ | grep -v "mock"

# 5. Commit and push
git add .
git commit -m "feat(v0.2-0X): [Feature name] implementation"
git push origin feature/v0.2-0X-*
```

---

## ðŸ“‹ GitHub CLI Commands (Post-Implementation)

When feature is complete:

```bash
# 1. Convert from draft to ready
gh pr ready 7 --repo danilivashyna/Atlas  # Replace 7 with your PR number

# 2. Add labels
gh pr edit 7 --repo danilivashyna/Atlas --add-label "v0.2" --add-label "ready-for-review"

# 3. Request review
gh pr edit 7 --repo danilivashyna/Atlas --add-reviewer danilivashyna

# 4. View PR
gh pr view 7 --repo danilivashyna/Atlas --web
```

---

## ðŸŽ¯ Quick Reference: Test Commands

```bash
# Run everything
pytest -q

# Run specific test file
pytest tests/test_api_smoke.py -q

# Run specific test
pytest tests/test_api_smoke.py::test_encode_h_returns_5d -v

# Run with output
pytest -q --tb=short

# Run with coverage
pytest --cov=src/atlas --cov-report=term-missing

# Stop on first failure
pytest -x

# Show print statements
pytest -s
```

---

## ðŸ“š Documentation Template (For Each Feature)

Each feature doc should include:

```markdown
# Feature Name (v0.2-0X)

## Overview
[2-3 sentences describing the feature]

## API/Usage
[Code examples]

## Formulas (if applicable)
[Mathematical descriptions]

## Examples
[Practical examples]

## References
[Links to relevant papers/docs]
```

---

**Last Updated**: 2025-10-20
**Status**: Ready for implementation
**Contact**: Raise questions in PR comments on GitHub
