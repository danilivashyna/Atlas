# v0.2 PR Body Templates

> Ready-to-copy PR descriptions for all 8 features
> **Format**: Copy each section and paste into GitHub PR body field

---

## PR #5 — TextEncoder5D

```markdown
## Goal
Implement TextEncoder5D: BERT 384D → PCA projection to 5D + normalization [-1,1] with fallback to SimpleSemanticEncoder.

## Scope
- `src/atlas/encoders/text_encoder_5d.py` - Main encoder implementation
- `src/atlas/encoders/__init__.py` - Export TextEncoder5D
- `src/atlas/api/app.py` - Integration via ATLAS_ENCODER_TYPE env var
- `tests/test_text_encoder_5d.py` - ≥22 tests for encoding, fallback, edge cases
- `README.md` - Usage example section

## Acceptance Criteria
- [x] `pytest -q` passes all tests (91+ baseline + new tests)
- [x] `/encode` endpoint works with `ATLAS_ENCODER_TYPE=TextEncoder5D` returning 5D vectors
- [x] Fallback to SimpleSemanticEncoder when sentence-transformers unavailable
- [x] PCA matrix stored as fixed weights in repo (no runtime training)
- [x] No new DeprecationWarnings
- [x] Type hints complete, docstrings present

## How to Test
```bash
export ATLAS_ENCODER_TYPE=TextEncoder5D
pytest tests/test_text_encoder_5d.py -v
pytest -q
uvicorn src.atlas.api.app:app --port 8010 &
curl -X POST http://localhost:8010/encode \
  -H "Content-Type: application/json" \
  -d '{"text":"hello world"}'
# Expected: 5D array in [-1, 1]
```

## Notes
- PCA matrix pre-trained offline, stored in repo (not in requirements)
- No external model downloads in CI (all offline)
- API contract unchanged, only output dimension differs
```

---

## PR #6 — Interpretable Transformer Decoder

```markdown
## Goal
Implement MVP interpretable decoder: from 5D → human-readable text + per-dimension reasoning extraction.

## Scope
- `src/atlas/decoders/interpretable_decoder.py` - Decoder + Reasoning classes
- `src/atlas/decoders/__init__.py` - Export InterpretableDecoder
- `tests/test_interpretable_decoder.py` - ≥10–13 tests for generation, reasoning, edge cases
- Documentation: inline docstrings explaining reasoning format

## Acceptance Criteria
- [x] `pytest -q` passes all tests
- [x] Reasoning format stable: list of `{dimension, weight/score, evidence}`
- [x] CPU performance acceptable: <120ms on short examples (not strict, but monitored)
- [x] No changes to `/decode` API contract
- [x] Type hints complete, docstrings for Reasoning structure

## How to Test
```bash
pytest tests/test_interpretable_decoder.py -v
pytest -q
python -c "
from src.atlas.decoders import InterpretableDecoder
decoder = InterpretableDecoder()
text, reasoning = decoder.decode([1, 0.5, -0.3, 0.2, -0.1])
print('Text:', text)
print('Reasoning:', reasoning)
"
```

## Notes
- Greedy or template-based generation (no heavy LLM)
- Focus on clarity of reasoning over perfect text
- Can use heuristics for dimension contribution
```

---

## PR #7 — Hierarchical API Endpoints

```markdown
## Goal
Add hierarchical API endpoints: `/encode_h`, `/decode_h`, `/explain_h` with Pydantic v2 models and OpenAPI docs.

## Scope
- `src/atlas/api/app.py` - New FastAPI routes with "Hierarchical" tags
- `src/atlas/api/models.py` - Pydantic v2 request/response schemas (EncodeHRequest, EncodeHResponse, etc.)
- `tests/test_api_smoke.py` - Smoke tests for new routes (status 200, correct schema)
- `docs/HIERARCHICAL_API.md` or README section - Payload/response examples

## Acceptance Criteria
- [x] `pytest -q` passes all tests
- [x] OpenAPI doc describes all parameters + includes examples
- [x] Responses include `trace_id` (UUID) and `timestamp` (UTC ISO 8601 "Z" format)
- [x] `/docs` (Swagger UI) renders all 3 new endpoints correctly
- [x] Pydantic v2 validation working (type checking, required fields)

## How to Test
```bash
pytest tests/test_api_smoke.py -v -k "encode_h or decode_h or explain_h"
pytest -q
uvicorn src.atlas.api.app:app --port 8010
# Visit: http://localhost:8010/docs
# Test endpoints via Swagger UI or:
curl -X POST http://localhost:8010/encode_h \
  -H "Content-Type: application/json" \
  -d '{"text":"hello","depth":2}'
```

## Notes
- Stubs acceptable for complex business logic (can implement in v0.2.1)
- Focus on API contract and OpenAPI correctness
- trace_id can be generated on-the-fly (uuid.uuid4())
```

---

## PR #8 — Hierarchical Losses

```markdown
## Goal
Implement differentiable loss functions for 5D hierarchical encoding: orthogonality, sparsity, entropy.

## Scope
- `src/atlas/training/losses.py` - `OrthogonalityLoss`, `SparsityLoss`, `RouterEntropyLoss` classes
- `tests/test_losses.py` - Correctness tests (formula verification), edge cases (zero, one-hot, etc.)
- `docs/LOSS_FUNCTIONS_v0.2.md` - Brief descriptions, formulas, usage examples

## Acceptance Criteria
- [x] `pytest -q` passes all tests (no flakes, deterministic)
- [x] Loss values reasonable: no NaN/Inf on valid inputs
- [x] Each loss function independent of ML framework (NumPy/PyTorch stub ok)
- [x] Numerical gradient checks passing (finite differences for verification)
- [x] Type hints complete, docstrings explaining formula

## How to Test
```bash
pytest tests/test_losses.py -v
pytest -q
python -c "
from src.atlas.training.losses import OrthogonalityLoss, SparsityLoss
loss_fn = OrthogonalityLoss()
import numpy as np
embedding = np.random.randn(5, 5)
loss = loss_fn(embedding)
print(f'Orthogonality loss: {loss}')
"
```

## Notes
- No full trainer integration yet (can add in v0.2.1)
- Focus on numerical stability and correctness
- Loss values should be scalar (aggregated across batch)
```

---

## PR #9 — Teacher → Student Distillation

```markdown
## Goal
Distillation scaffold: load teacher model, load student model, training step, simple metric tracking, CLI tool.

## Scope
- `src/atlas/training/distill.py` - `Distiller` class with `step()`, `load_teacher()`, `load_student()`, `save()`
- `src/atlas/training/configs.py` - `DistillerConfig` dataclass (paths, hyperparams, learning rate, etc.)
- `tools/train_distill.py` - CLI with `--teacher`, `--student`, `--epochs`, `--batch-size`, `--output-dir`
- `docs/distillation_quickstart.md` - Quick start guide
- `tests/test_distill_smoke.py` - Dry-run 1–2 iterations on CPU, no model downloads

## Acceptance Criteria
- [x] `pytest -q` passes without network (all deps local or stubbed)
- [x] CLI runs: `python tools/train_distill.py --help` shows all options
- [x] Training loop executes 1–2 steps without crashing
- [x] Logs printed to stdout, artifacts saved to `./runs/` directory
- [x] Graceful handling of missing/unavailable models (skip or use dummy)

## How to Test
```bash
pytest tests/test_distill_smoke.py -v
pytest -q
python tools/train_distill.py --help
# Dry-run:
python tools/train_distill.py --epochs 1 --batch-size 2 --output-dir ./test_runs
# Check output:
ls -la ./test_runs/
```

## Notes
- No large-scale training in v0.2 (dry-run only)
- Focus on CLI structure and training loop flow
- Can use simplified/dummy teacher/student for testing
- Real models can be loaded in v0.2.1
```

---

## PR #10 — Hierarchical Metrics

```markdown
## Goal
Implement hierarchical metrics: H-Coherence, H-Stability, H-Diversity for measuring embedding quality.

## Scope
- `src/atlas/metrics/metrics_hier.py` - `coherence()`, `stability()`, `diversity()` functions
- `tests/test_metrics_hier.py` - Deterministic test cases, edge cases (empty batch, uniform embeddings)
- `docs/METRICS_v0.2.md` - Formulas, interpretation, calculation examples on small arrays

## Acceptance Criteria
- [x] `pytest -q` passes all tests
- [x] Metrics are pure functions (no external models, NumPy-based)
- [x] Output values in sensible range: 0 ≤ value ≤ 1 (or documented otherwise)
- [x] Documentation includes manual calculation examples for verification
- [x] Type hints complete, docstrings explain formula and interpretation

## How to Test
```bash
pytest tests/test_metrics_hier.py -v
pytest -q
python -c "
from src.atlas.metrics.metrics_hier import coherence, stability, diversity
import numpy as np
embeddings = np.random.randn(10, 5)  # 10 samples, 5D
c = coherence(embeddings)
s = stability(embeddings)
d = diversity(embeddings)
print(f'Coherence: {c}, Stability: {s}, Diversity: {d}')
"
```

## Notes
- All metrics should output scalar value (aggregated over batch)
- No heavy dependencies for metrics computation
- Can visualize later (not in v0.2)
```

---

## PR #11 — Benchmarks

```markdown
## Goal
Add benchmark scenarios for encoding latency, decoding speed, and reasoning extraction using pytest-benchmark.

## Scope
- `tests/benchmarks/test_bench_encoder.py` - Latency benchmarks for encoder
- `tests/benchmarks/test_bench_decoder.py` - Speed benchmarks for decoder and reasoning
- Integration with pytest-benchmark (skip gracefully if not installed)
- `docs/BENCHMARKS.md` - How to run benchmarks and interpret results

## Acceptance Criteria
- [x] `pytest -q` passes; benchmarks skipped without pytest-benchmark package
- [x] Clear benchmark parameters: input sizes, repetition counts, CPU/GPU mode
- [x] No flaky/unstable dependencies or measurements
- [x] Results reproducible on same hardware
- [x] Baseline numbers documented (reference for future optimization)

## How to Test
```bash
pytest tests/benchmarks/ -v
pytest -q
# If pytest-benchmark installed:
pytest tests/benchmarks/ --benchmark-only
# View results:
pytest tests/benchmarks/ --benchmark-compare
```

## Notes
- Skipping is acceptable if pytest-benchmark not available
- Focus on consistent measurements, not absolute numbers
- No CI artifact publishing (local testing only in v0.2)
```

---

## PR #12 — Docs, Demos, CLI

```markdown
## Goal
Update documentation and add CLI tool with `encode`, `decode`, `explain` commands for end-user interaction.

## Scope
- `docs/QUICK_REFERENCE.md`, `docs/v0.2_DEVELOPMENT_STATUS.md`, `README.md` - Updated sections with v0.2 features
- `tools/atlas_cli.py` - CLI script with `encode`, `decode`, `explain` subcommands
- Command examples in docs or in `--help` output
- `tests/test_cli_smoke.py` - Smoke tests: CLI runs, returns expected format

## Acceptance Criteria
- [x] `pytest -q` passes all tests
- [x] CLI works without network/external model downloads
- [x] `python tools/atlas_cli.py --help` displays all commands and options
- [x] Command examples in docs are current and copy-paste ready
- [x] Each subcommand has `-h/--help` with clear usage

## How to Test
```bash
pytest tests/test_cli_smoke.py -v
pytest -q
python tools/atlas_cli.py --help
python tools/atlas_cli.py encode "hello world"
python tools/atlas_cli.py decode "[1, 0.5, -0.3, 0.2, -0.1]"
python tools/atlas_cli.py explain "[1, 0.5, -0.3, 0.2, -0.1]"
```

## Notes
- CLI should work offline (no model downloads)
- No Jupyter/Colab in v0.2 (future work with link)
- Focus on accessibility for end-users (clear help, good defaults)
```

---

# 🎯 General Template (Copy & Customize)

If you need to write your own PR description:

```markdown
## Goal
[What is implemented, in 1-2 sentences]

## Scope
- [File/module changes]
- [Test file]
- [Documentation updates]

## Acceptance Criteria
- [x] pytest green
- [x] No new warnings
- [x] API contract maintained (or list changes)
- [x] [Feature-specific criterion]

## How to Test
```bash
[Command 1]
[Command 2]
```

## Notes
- [Any risks or limitations]
- [Dependencies on other features]
```

---

## 📋 Copy Instructions

1. **For PR Body**:
   - Click "Edit" on the PR page
   - Clear existing body
   - Copy the markdown section for your PR
   - Click "Save"

2. **Via GitHub CLI**:
   ```bash
   # Create file with content:
   echo "[CONTENT HERE]" > pr_body.md

   # Apply to PR:
   gh pr edit 5 --body-file pr_body.md
   ```

3. **Manual**:
   - Open this file
   - Find your PR number
   - Copy the markdown block
   - Paste into GitHub PR body

---

**Last Updated**: 2025-10-20
